{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS5559 - Project\n",
    "## Notebook 1 - Import Corpus\n",
    "#### Name: Mengyao Zhang (mz6jv), Runhao Zhao (rz6dg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "Use case: import raw text, process and then save in F3 form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mz6jv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mz6jv/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/mz6jv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/mz6jv/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mz6jv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define OHCO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHCO for our corpus\n",
    "# Since there are multiple books, we added book_num.\n",
    "OHCO = [\"book_num\",'chap_num', 'para_num', 'sent_num', 'token_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert text to tokens\n",
    "def text_to_tokens(src_file,\n",
    "                   body_start=0, \n",
    "                   body_end=-1,\n",
    "                   book_pat =r'^\\s*ClassicBook.*$',  \n",
    "                   chap_pat=r'^\\s*Chapter.*$', \n",
    "                   para_pat=r'\\n\\n+', \n",
    "                   sent_pat=r'([.;?!\"“”]+)', \n",
    "                   token_pat=r'([\\W_]+)'):\n",
    "\n",
    "    # text to lines\n",
    "    lines = open(src_file, 'r', encoding='utf-8').readlines()\n",
    "    lines = lines[body_start - 1 : body_end + 1]\n",
    "    df = pd.DataFrame({'line_str':lines})\n",
    "    df.index.name = 'line_id'\n",
    "    df.line_str = df.line_str.str.replace('—', ' — ')\n",
    "    df.line_str = df.line_str.str.replace('-', ' - ')\n",
    "    del(lines)\n",
    "    \n",
    "    # lines to books\n",
    "    mask = df.line_str.str.match(book_pat)\n",
    "    df.loc[mask, 'book_id'] = df.apply(lambda x: x.name, 1)\n",
    "    df.book_id = df.book_id.ffill()\n",
    "    df.book_id = df.book_id.fillna(method=\"bfill\")\n",
    "    book_ids = df.book_id.unique().tolist()\n",
    "    df['book_num'] = df.book_id.apply(lambda x: book_ids.index(x)+1)\n",
    "\n",
    "    mask = df.line_str.str.match(chap_pat)\n",
    "    df.loc[mask, 'chap_id'] = df.apply(lambda x: x.name, 1)\n",
    "    df.chap_id = df.chap_id.ffill()\n",
    "    df.chap_id = df.chap_id.fillna(method=\"bfill\")\n",
    "    chap_ids = df.chap_id.unique().tolist()\n",
    "    \n",
    "    # books to chaps    \n",
    "    df['chap_num'] = df.chap_id.apply(lambda x: chap_ids.index(x)+1)\n",
    "    df.drop([\"book_id\",\"chap_id\"],axis=1,inplace=True)\n",
    "    books = df.groupby(['book_num',\"chap_num\"])\\\n",
    "        .apply(lambda x:''.join(x.line_str))\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'chap_str'})\n",
    "    \n",
    "    chaps = books.reset_index('chap_num', drop=True)\n",
    "    chaps = chaps.set_index(books.groupby(level=0).cumcount().rename('chap_num'), append=True)\n",
    "    del(df)\n",
    "\n",
    "    # chapters to paragraphs\n",
    "    paras = chaps.chap_str.str.split(para_pat, expand=True)\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'para_str'})\n",
    "    paras.index.names = OHCO[:3] #['chap_num', 'para_num']\n",
    "    paras.para_str = paras.para_str.str.strip()\n",
    "    paras.para_str = paras.para_str.str.replace(r'\\'', ' ')\n",
    "    paras.para_str = paras.para_str.str.replace(r'_', ' ')\n",
    "#     paras.para_str = paras.para_str.str.replace('é', 'e')\n",
    "#     paras.para_str = paras.para_str.str.replace('à', 'a')\n",
    "#     paras.para_str = paras.para_str.str.replace('è', 'e')\n",
    "#     paras.para_str = paras.para_str.str.replace('ù', 'u')\n",
    "#     paras.para_str = paras.para_str.str.replace('â', 'a')\n",
    "#     paras.para_str = paras.para_str.str.replace('ê', 'e')\n",
    "#     paras.para_str = paras.para_str.str.replace('î', 'i')\n",
    "#     paras.para_str = paras.para_str.str.replace('ô', 'o')\n",
    "#     paras.para_str = paras.para_str.str.replace('û', 'u')\n",
    "#     paras.para_str = paras.para_str.str.replace('ç', 'c')\n",
    "    paras.para_str = paras.para_str.str.replace(r'\\n', ' ')\n",
    "    paras.para_str = paras.para_str.str.replace(r'\\s+', ' ')\n",
    "    paras = paras[~paras.para_str.str.match(r'^\\s*$')]\n",
    "    del(chaps)\n",
    "    \n",
    "    # paragraphs to sentences\n",
    "    sents = paras.para_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'sent_str'})\n",
    "    sents.index.names = OHCO[:4]\n",
    "    del(paras)\n",
    "    \n",
    "    # sentences to tokens\n",
    "    tokens = sents.sent_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'pos_tuple'})\n",
    "    tokens.index.names = OHCO #['chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "    tokens['pos'] = tokens.pos_tuple.apply(lambda x: x[1])\n",
    "    tokens['token_str'] = tokens.pos_tuple.apply(lambda x: x[0])\n",
    "    tokens = tokens.drop('pos_tuple', 1)\n",
    "    del(sents)\n",
    "    \n",
    "   \n",
    "\n",
    "    # Tag punction\n",
    "    tokens['punc'] = tokens.token_str.str.match(r'^[\\W_]*$').astype('int')\n",
    "    tokens['num'] = tokens.token_str.str.match(r'\\d').astype('int')\n",
    "    \n",
    "    # Extract vocab\n",
    "    WORDS = (tokens.punc == 0) & (tokens.num == 0)\n",
    "    tokens.loc[WORDS, 'term_str'] = tokens.token_str.str.lower()\n",
    "    vocab = tokens[tokens.punc == 0].term_str.value_counts().to_frame()\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={'index':'term_str', 'term_str':'n'})\n",
    "    vocab = vocab.sort_values('term_str').reset_index()\n",
    "    vocab.index.name = 'term_id'\n",
    "    vocab = vocab.drop('index', 1)\n",
    "        \n",
    "    # Add term_ids to tokens \n",
    "    tokens['term_id'] = tokens['term_str'].map(vocab.reset_index()\\\n",
    "        .set_index('term_str').term_id).fillna(-1).astype('int')\n",
    "\n",
    "    return tokens, vocab\n",
    "\n",
    "def get_docs(tokens, div_names, doc_str = 'term_id', sep='', flatten=False, \n",
    "             index_only=False):\n",
    "    \n",
    "    if not index_only:\n",
    "        docs = tokens.groupby(div_names)[doc_str]\\\n",
    "          .apply(lambda x: x.str.cat(sep=sep))\n",
    "        docs.columns = ['doc_content']\n",
    "    else:\n",
    "        docs = tokens.groupby(div_names)[doc_str].apply(lambda x: x.tolist())\n",
    "\n",
    "    if flatten:\n",
    "        docs = docs.reset_index().drop(div_names, 1)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def get_term_id(vocab, term_str):\n",
    "    return vocab[vocab.term_str == term_str].index[0]\n",
    "\n",
    "def get_term_str(vocab, term_id):\n",
    "    return vocab.loc[term_id].term_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in source file and apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = './combined_again.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "cfg = dict(\n",
    "    src_file = src_file,\n",
    "    body_start = 3,\n",
    "    body_end = 642122\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the text_to_tokens() function to get token and vocabulary tables\n",
    "K,V= text_to_tokens(**cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of chapters\n",
    "len(K.reset_index(level=[0,1,2,3]).groupby([\"book_num\",\"chap_num\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109472"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of paragraphs\n",
    "len(K.reset_index(level=[0,1,2,3]).groupby([\"book_num\",\"chap_num\",\"para_num\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of sentences\n",
    "len(K.reset_index(level=[0,1,2,3]).groupby([\"book_num\",\"chap_num\",\"para_num\",\"sent_num\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>punc</th>\n",
       "      <th>num</th>\n",
       "      <th>term_str</th>\n",
       "      <th>term_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_num</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">13</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">514</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">167</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">2</th>\n",
       "      <th>32</th>\n",
       "      <td>VBG</td>\n",
       "      <td>rising</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>rising</td>\n",
       "      <td>28055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NN</td>\n",
       "      <td>sigh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sigh</td>\n",
       "      <td>29861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>of</td>\n",
       "      <td>22650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>VBG</td>\n",
       "      <td>repining</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>repining</td>\n",
       "      <td>27419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NN</td>\n",
       "      <td>mortality</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mortality</td>\n",
       "      <td>21473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>and</td>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>JJ</td>\n",
       "      <td>grateful</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>grateful</td>\n",
       "      <td>14697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>IN</td>\n",
       "      <td>with</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>with</td>\n",
       "      <td>37125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>JJ</td>\n",
       "      <td>general</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>general</td>\n",
       "      <td>14079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NN</td>\n",
       "      <td>felicity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>felicity</td>\n",
       "      <td>12553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RB</td>\n",
       "      <td>bore</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bore</td>\n",
       "      <td>3640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>JJ</td>\n",
       "      <td>partial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>partial</td>\n",
       "      <td>23635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NN</td>\n",
       "      <td>evil</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>evil</td>\n",
       "      <td>11669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>IN</td>\n",
       "      <td>with</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>with</td>\n",
       "      <td>37125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>JJ</td>\n",
       "      <td>chearfullest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chearfullest</td>\n",
       "      <td>5262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NN</td>\n",
       "      <td>resignation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>resignation</td>\n",
       "      <td>27589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               pos     token_str  punc  num  \\\n",
       "book_num chap_num para_num sent_num token_num                                 \n",
       "13       514      167      2        32         VBG        rising     0    0   \n",
       "                                    33          NN          sigh     0    0   \n",
       "                                    34          IN            of     0    0   \n",
       "                                    35         VBG      repining     0    0   \n",
       "                                    36          NN     mortality     0    0   \n",
       "                                    37           ,             ,     1    0   \n",
       "                                    38          CC           and     0    0   \n",
       "                                    39           ,             ,     1    0   \n",
       "                                    40          JJ      grateful     0    0   \n",
       "                                    41          IN          with     0    0   \n",
       "                                    42          JJ       general     0    0   \n",
       "                                    43          NN      felicity     0    0   \n",
       "                                    44           ,             ,     1    0   \n",
       "                                    45          RB          bore     0    0   \n",
       "                                    46          JJ       partial     0    0   \n",
       "                                    47          NN          evil     0    0   \n",
       "                                    48          IN          with     0    0   \n",
       "                                    49          JJ  chearfullest     0    0   \n",
       "                                    50          NN   resignation     0    0   \n",
       "                                    51           .             .     1    0   \n",
       "\n",
       "                                                   term_str  term_id  \n",
       "book_num chap_num para_num sent_num token_num                         \n",
       "13       514      167      2        32               rising    28055  \n",
       "                                    33                 sigh    29861  \n",
       "                                    34                   of    22650  \n",
       "                                    35             repining    27419  \n",
       "                                    36            mortality    21473  \n",
       "                                    37                  NaN       -1  \n",
       "                                    38                  and     1156  \n",
       "                                    39                  NaN       -1  \n",
       "                                    40             grateful    14697  \n",
       "                                    41                 with    37125  \n",
       "                                    42              general    14079  \n",
       "                                    43             felicity    12553  \n",
       "                                    44                  NaN       -1  \n",
       "                                    45                 bore     3640  \n",
       "                                    46              partial    23635  \n",
       "                                    47                 evil    11669  \n",
       "                                    48                 with    37125  \n",
       "                                    49         chearfullest     5262  \n",
       "                                    50          resignation    27589  \n",
       "                                    51                  NaN       -1  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37712, 2)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of terms in vocabulary\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further process vocab table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add priors to vocab table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V['p'] = V.n / V.n.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add stems to vocab table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "V['port_stem'] = V.term_str.apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add stopwords flag to vocab table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "sw = pd.DataFrame({'x':1}, index=stopwords)\n",
    "V['stop'] = V.term_str.map(sw.x).fillna(0).astype('int')\n",
    "del(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = 'project.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(db_file) as db:\n",
    "    K.to_sql('token', db, if_exists='replace', index=True)\n",
    "    V.to_sql('vocab', db, if_exists='replace', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
